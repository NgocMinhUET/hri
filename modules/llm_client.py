"""
LLM Client - T√≠ch h·ª£p Gemini API cho kh·∫£ nƒÉng h·ªôi tho·∫°i
"""
import google.generativeai as genai
from typing import List, Dict, Optional
from utils.logger import setup_logger
from utils.config_loader import get_config

class LLMClient:
    """Client ƒë·ªÉ giao ti·∫øp v·ªõi Gemini LLM"""
    
    def __init__(self, config=None):
        """
        Args:
            config: ConfigLoader instance
        """
        self.config = config or get_config()
        self.logger = setup_logger("LLM")
        
        # Load c·∫•u h√¨nh
        api_key = self.config.get('llm.api_key')
        if not api_key:
            raise ValueError("Ch∆∞a c·∫•u h√¨nh GEMINI_API_KEY! H√£y th√™m v√†o file .env")
        
        self.model_name = self.config.get('llm.model', 'gemini-1.5-flash')
        self.temperature = self.config.get('llm.temperature', 0.7)
        self.max_tokens = self.config.get('llm.max_tokens', 150)
        self.system_prompt = self.config.get('llm.system_prompt', '')
        
        # C·∫•u h√¨nh Gemini
        genai.configure(api_key=api_key)
        
        # Kh·ªüi t·∫°o model
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config={
                'temperature': self.temperature,
                'max_output_tokens': self.max_tokens,
            }
        )
        
        # Conversation history
        self.conversation_history: List[Dict[str, str]] = []
        
        # B·∫Øt ƒë·∫ßu chat session
        self.chat = self.model.start_chat(history=[])
        
        self.logger.info(f"LLM Client ƒë√£ s·∫µn s√†ng! Model: {self.model_name}")
    
    def generate_response(self, user_message: str) -> str:
        """
        T·∫°o response t·ª´ user message
        
        Args:
            user_message: Tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng
        
        Returns:
            Response t·ª´ LLM
        """
        try:
            # Th√™m system prompt n·∫øu ƒë√¢y l√† tin nh·∫Øn ƒë·∫ßu ti√™n
            if not self.conversation_history and self.system_prompt:
                prompt = f"{self.system_prompt}\n\nUser: {user_message}"
            else:
                prompt = user_message
            
            self.logger.info(f"üë§ User: {user_message}")
            
            # G·ª≠i message
            response = self.chat.send_message(prompt)
            bot_response = response.text.strip()
            
            # L∆∞u history
            self.conversation_history.append({
                'role': 'user',
                'content': user_message
            })
            self.conversation_history.append({
                'role': 'assistant',
                'content': bot_response
            })
            
            self.logger.info(f"ü§ñ Bot: {bot_response}")
            
            return bot_response
            
        except Exception as e:
            self.logger.error(f"L·ªói khi g·ªçi Gemini API: {e}")
            return "Xin l·ªói, t√¥i ƒëang g·∫∑p v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t. B·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i kh√¥ng?"
    
    def reset_conversation(self):
        """Reset cu·ªôc h·ªôi tho·∫°i"""
        self.conversation_history = []
        self.chat = self.model.start_chat(history=[])
        self.logger.info("ƒê√£ reset cu·ªôc h·ªôi tho·∫°i.")
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """L·∫•y l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        return self.conversation_history.copy()
    
    def set_system_prompt(self, prompt: str):
        """C·∫≠p nh·∫≠t system prompt"""
        self.system_prompt = prompt
        self.logger.info(f"ƒê√£ c·∫≠p nh·∫≠t system prompt: {prompt[:50]}...")

# Test standalone
if __name__ == "__main__":
    import sys
    
    try:
        llm = LLMClient()
        
        print("\nü§ñ Gemini LLM Test")
        print("=" * 50)
        print("H√£y chat v·ªõi bot!")
        print("(G√µ 'exit' ho·∫∑c 'quit' ƒë·ªÉ tho√°t)")
        print("(G√µ 'reset' ƒë·ªÉ reset cu·ªôc h·ªôi tho·∫°i)\n")
        
        while True:
            user_input = input("üë§ B·∫°n: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['exit', 'quit', 'tho√°t']:
                print("T·∫°m bi·ªát!")
                break
            
            if user_input.lower() == 'reset':
                llm.reset_conversation()
                print("‚úÖ ƒê√£ reset cu·ªôc h·ªôi tho·∫°i.\n")
                continue
            
            response = llm.generate_response(user_input)
            print(f"ü§ñ Bot: {response}\n")
        
    except KeyboardInterrupt:
        print("\nTho√°t ch∆∞∆°ng tr√¨nh.")
    except Exception as e:
        print(f"L·ªói: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

